{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afbeced0",
   "metadata": {},
   "source": [
    "### Creating your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c8d24de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.github.com/repos/huggingface/datasets/issues?page=1&per_page=1\"\n",
    "response = requests.get(url)\n",
    "\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c0e9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/7999',\n",
       "  'repository_url': 'https://api.github.com/repos/huggingface/datasets',\n",
       "  'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/7999/labels{/name}',\n",
       "  'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/7999/comments',\n",
       "  'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/7999/events',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/issues/7999',\n",
       "  'id': 3915367642,\n",
       "  'node_id': 'I_kwDODunzps7pX8Ta',\n",
       "  'number': 7999,\n",
       "  'title': 'Too many dataloader workers: 4 (max is dataset.num_shards=3). Stopping 1 dataloader workers.',\n",
       "  'user': {'login': 'D222097',\n",
       "   'id': 50061868,\n",
       "   'node_id': 'MDQ6VXNlcjUwMDYxODY4',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/50061868?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/D222097',\n",
       "   'html_url': 'https://github.com/D222097',\n",
       "   'followers_url': 'https://api.github.com/users/D222097/followers',\n",
       "   'following_url': 'https://api.github.com/users/D222097/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/D222097/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/D222097/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/D222097/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/D222097/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/D222097/repos',\n",
       "   'events_url': 'https://api.github.com/users/D222097/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/D222097/received_events',\n",
       "   'type': 'User',\n",
       "   'user_view_type': 'public',\n",
       "   'site_admin': False},\n",
       "  'labels': [],\n",
       "  'state': 'open',\n",
       "  'locked': False,\n",
       "  'assignee': None,\n",
       "  'assignees': [],\n",
       "  'milestone': None,\n",
       "  'comments': 0,\n",
       "  'created_at': '2026-02-09T09:26:37Z',\n",
       "  'updated_at': '2026-02-09T10:13:25Z',\n",
       "  'closed_at': None,\n",
       "  'author_association': 'NONE',\n",
       "  'type': None,\n",
       "  'active_lock_reason': None,\n",
       "  'sub_issues_summary': {'total': 0, 'completed': 0, 'percent_completed': 0},\n",
       "  'issue_dependencies_summary': {'blocked_by': 0,\n",
       "   'total_blocked_by': 0,\n",
       "   'blocking': 0,\n",
       "   'total_blocking': 0},\n",
       "  'body': 'Hi !\\nIâ€™m working on training with a large-scale dataset (100+ Parquet files) using lazy loading, and Iâ€™m struggling to understand/optimize the num_shards settingâ€” in the lerobot repo: streaming_datasets.py:\\n```\\nfrom datasets import load_dataset\\nself.hf_dataset: datasets.IterableDataset = load_dataset(\\n            self.repo_id if not self.streaming_from_local else str(self.root),\\n            split=\"train\",\\n            streaming=self.streaming,\\n            data_files=\"data/*/*.parquet\",\\n            revision=self.revision,\\n        )\\n\\nself.num_shards = min(self.hf_dataset.num_shards, max_num_shards)\\n```\\n\\n```\\ndataloader = torch.utils.data.DataLoader(\\n            datasets[sub_idx],\\n            num_workers=datasets[sub_idx].num_shards, #cfg.num_workers,\\n            batch_size=cfg.batch_size,\\n            shuffle=shuffle and not cfg.dataset.streaming,\\n            sampler=sampler,\\n            collate_fn=FlowerDataCollator(),\\n            pin_memory=device.type == \"cuda\",\\n            drop_last=True,\\n            prefetch_factor=2 if cfg.num_workers > 0 else None,\\n            )\\n```\\n\\nWhat exactly does hf_dataset.**num_shards** represent? Is it safe to manually override/edit num_shards?\\n\\nMy batch loading is slower than expected (2-3s per batch) despite num_worker cannot be bigger with warning: Too many dataloader workers: 4 (max is dataset.num_shards=3). Stopping 1 dataloader workers. \\n\\nEven use num_workers=datasets[sub_idx].num_shards, the waring is still exist! (my num_worker is 4 and hf_dataset.num_shards is 100+, so the datasets[sub_idx].num_shards=4)\\nWhy does the \"too many workers\" warning persist even when num_workers equals dataset.num_shardsâ€”and how do I fix this?\\n\\nThanks so much for any insights or help with this! Really appreciate your time and expertise ðŸ˜Š\\n\\n',\n",
       "  'closed_by': None,\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/7999/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/7999/timeline',\n",
       "  'performed_via_github_app': None,\n",
       "  'state_reason': None}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac965f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GITHUB_TOKEN = \"\"\n",
    "# headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b7ad27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def fetch_issues(\n",
    "    owner=\"huggingface\",\n",
    "    repo=\"datasets\",\n",
    "    num_issues=5_000,\n",
    "    rate_limit=5_5000,\n",
    "    issues_path=Path(\"Data/\")\n",
    "):\n",
    "    if not issues_path.is_dir():\n",
    "        issues_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    batch = []\n",
    "    all_issues = []\n",
    "    per_page = 100\n",
    "    num_pages = math.ceil(num_issues/per_page)\n",
    "    base_url = \"https://api.github.com/repos\"\n",
    "\n",
    "    for page in tqdm(range(num_pages)):\n",
    "        query = f\"issues?page={page}&per_page={per_page}&state=all\"\n",
    "        issues = requests.get(f\"{base_url}/{owner}/{repo}/{query}\", headers=headers)\n",
    "        batch.extend(issues.json())\n",
    "\n",
    "        if len(batch) > rate_limit and len(all_issues) < num_issues:\n",
    "            all_issues.extend(batch)\n",
    "            batch = []\n",
    "            print(f\"Reached Github rate limit. Sleeping for one hour ... \")\n",
    "            time.sleep(60*60+1)\n",
    "\n",
    "    all_issues.extend(batch)\n",
    "    df = pd.DataFrame.from_records(all_issues)\n",
    "    df.to_json(f\"{issues_path}/{repo}-issues.jsonl\", orient=\"records\", lines=True)\n",
    "    print(\n",
    "        f\"Downloaded all the issues for {repo}! Dataset stored at {issues_path}/{repo}-issues.jsonl\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0eecf7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac1a1e47e4441c6a1ea4474b7ce4600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded all the issues for datasets! Dataset stored at Data/datasets-issues.jsonl\n"
     ]
    }
   ],
   "source": [
    "fetch_issues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47ab8265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'type', 'active_lock_reason', 'sub_issues_summary', 'issue_dependencies_summary', 'body', 'closed_by', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'draft', 'pull_request'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load issues using load dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "issues_dataset = load_dataset(\"json\", data_files=\"Data/datasets-issues.jsonl\", split=\"train\")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98d3287",
   "metadata": {},
   "source": [
    "## Cleaning up the data\n",
    "\n",
    "Since the contents of issues and pull requests are quite different, letâ€™s do some minor preprocessing to enable us to distinguish between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d0f7e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> URL: https://github.com/huggingface/datasets/issues/7281\n",
      ">> PR: None\n",
      ">> URL: https://github.com/huggingface/datasets/pull/4724\n",
      ">> PR: {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/4724', 'html_url': 'https://github.com/huggingface/datasets/pull/4724', 'diff_url': 'https://github.com/huggingface/datasets/pull/4724.diff', 'patch_url': 'https://github.com/huggingface/datasets/pull/4724.patch', 'merged_at': datetime.datetime(2022, 9, 5, 17, 25, 27)}\n",
      ">> URL: https://github.com/huggingface/datasets/pull/7120\n",
      ">> PR: {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/7120', 'html_url': 'https://github.com/huggingface/datasets/pull/7120', 'diff_url': 'https://github.com/huggingface/datasets/pull/7120.diff', 'patch_url': 'https://github.com/huggingface/datasets/pull/7120.patch', 'merged_at': datetime.datetime(2024, 8, 22, 14, 33, 52)}\n"
     ]
    }
   ],
   "source": [
    "sample = issues_dataset.shuffle(seed=666).select(range(3))\n",
    "\n",
    "for url, pr in zip(sample[\"html_url\"], sample[\"pull_request\"]):\n",
    "    print(f\">> URL: {url}\")\n",
    "    print(f\">> PR: {pr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d37c5aa",
   "metadata": {},
   "source": [
    "Here we can see that each pull request is associated with various URLs, while ordinary issues have a None entry. We can use this distinction to create a new is_pull_request column that checks whether the pull_request field is None or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "855ccdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_dataset = issues_dataset.map((lambda x: {\"is_pull_request\": False if  x[\"pull_request\"] is None else True}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d275fec",
   "metadata": {},
   "source": [
    "### Augumenting the dataset\n",
    "the comments associated with an issue or pull request provide a rich source of information, especially if weâ€™re interested in building a search engine to answer user queries about the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8705d2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/897594128',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/2792#issuecomment-897594128',\n",
       "  'issue_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792',\n",
       "  'id': 897594128,\n",
       "  'node_id': 'IC_kwDODunzps41gDMQ',\n",
       "  'user': {'login': 'bhavitvyamalik',\n",
       "   'id': 19718818,\n",
       "   'node_id': 'MDQ6VXNlcjE5NzE4ODE4',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/19718818?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/bhavitvyamalik',\n",
       "   'html_url': 'https://github.com/bhavitvyamalik',\n",
       "   'followers_url': 'https://api.github.com/users/bhavitvyamalik/followers',\n",
       "   'following_url': 'https://api.github.com/users/bhavitvyamalik/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/bhavitvyamalik/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/bhavitvyamalik/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/bhavitvyamalik/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/bhavitvyamalik/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/bhavitvyamalik/repos',\n",
       "   'events_url': 'https://api.github.com/users/bhavitvyamalik/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/bhavitvyamalik/received_events',\n",
       "   'type': 'User',\n",
       "   'user_view_type': 'public',\n",
       "   'site_admin': False},\n",
       "  'created_at': '2021-08-12T12:21:52Z',\n",
       "  'updated_at': '2021-08-12T12:31:17Z',\n",
       "  'body': \"@albertvillanova my tests are failing here:\\r\\n```\\r\\ndataset_name = 'gooaq'\\r\\n\\r\\n    def test_load_dataset(self, dataset_name):\\r\\n        configs = self.dataset_tester.load_all_configs(dataset_name, is_local=True)[:1]\\r\\n>       self.dataset_tester.check_load_dataset(dataset_name, configs, is_local=True, use_local_dummy_data=True)\\r\\n\\r\\ntests/test_dataset_common.py:234: \\r\\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \\r\\ntests/test_dataset_common.py:187: in check_load_dataset\\r\\n    self.parent.assertTrue(len(dataset[split]) > 0)\\r\\nE   AssertionError: False is not true\\r\\n```\\r\\nWhen I try loading dataset on local machine it works fine. Any suggestions on how can I avoid this error?\",\n",
       "  'author_association': 'CONTRIBUTOR',\n",
       "  'pin': None,\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/897594128/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'performed_via_github_app': None},\n",
       " {'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/898644889',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/2792#issuecomment-898644889',\n",
       "  'issue_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792',\n",
       "  'id': 898644889,\n",
       "  'node_id': 'IC_kwDODunzps41kDuZ',\n",
       "  'user': {'login': 'bhavitvyamalik',\n",
       "   'id': 19718818,\n",
       "   'node_id': 'MDQ6VXNlcjE5NzE4ODE4',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/19718818?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/bhavitvyamalik',\n",
       "   'html_url': 'https://github.com/bhavitvyamalik',\n",
       "   'followers_url': 'https://api.github.com/users/bhavitvyamalik/followers',\n",
       "   'following_url': 'https://api.github.com/users/bhavitvyamalik/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/bhavitvyamalik/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/bhavitvyamalik/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/bhavitvyamalik/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/bhavitvyamalik/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/bhavitvyamalik/repos',\n",
       "   'events_url': 'https://api.github.com/users/bhavitvyamalik/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/bhavitvyamalik/received_events',\n",
       "   'type': 'User',\n",
       "   'user_view_type': 'public',\n",
       "   'site_admin': False},\n",
       "  'created_at': '2021-08-13T18:28:27Z',\n",
       "  'updated_at': '2021-08-13T18:28:27Z',\n",
       "  'body': 'Thanks for the help, @albertvillanova! All tests are passing now.',\n",
       "  'author_association': 'CONTRIBUTOR',\n",
       "  'pin': None,\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/898644889/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'performed_via_github_app': None}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issue_number = 2792\n",
    "url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
    "response = requests.get(url, headers=headers)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ef210d",
   "metadata": {},
   "source": [
    "We can see that the comment is stored in the body field, so letâ€™s write a simple function that returns all the comments associated with an issue by picking out the body contents for each element in response.json():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e3f51885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"@albertvillanova my tests are failing here:\\r\\n```\\r\\ndataset_name = 'gooaq'\\r\\n\\r\\n    def test_load_dataset(self, dataset_name):\\r\\n        configs = self.dataset_tester.load_all_configs(dataset_name, is_local=True)[:1]\\r\\n>       self.dataset_tester.check_load_dataset(dataset_name, configs, is_local=True, use_local_dummy_data=True)\\r\\n\\r\\ntests/test_dataset_common.py:234: \\r\\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \\r\\ntests/test_dataset_common.py:187: in check_load_dataset\\r\\n    self.parent.assertTrue(len(dataset[split]) > 0)\\r\\nE   AssertionError: False is not true\\r\\n```\\r\\nWhen I try loading dataset on local machine it works fine. Any suggestions on how can I avoid this error?\",\n",
       " 'Thanks for the help, @albertvillanova! All tests are passing now.']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_comments(issue_number):\n",
    "    \n",
    "    url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        return []\n",
    "    \n",
    "    # Parse JSON response\n",
    "    data = response.json()\n",
    "    \n",
    "    # Ensure data is a list before trying to iterate\n",
    "    if not isinstance(data, list):\n",
    "        return []\n",
    "    \n",
    "    # Extract comments with additional safety check\n",
    "    return [r[\"body\"] for r in data if isinstance(r, dict) and \"body\" in r]\n",
    "\n",
    "\n",
    "# Test our function works as expected\n",
    "get_comments(2792)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7190af",
   "metadata": {},
   "source": [
    " so letâ€™s use Dataset.map() to add a new comments column to each issue in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a90715ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c1565951974bf4ad2d70ebbd862834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "issues_with_comments_dataset = issues_dataset.map(\n",
    "    lambda x: {\"comments\": get_comments(x[\"number\"])}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cc2ac36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdbe0d851af341cdbab9d15b807b6cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9955f601892413fbd157284e3984e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44969c24085f4ff08bae8a9852020407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b13aa835f6404986165aa227c04e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/DrSly/github-issues/commit/0c2874fb4afc42bf8177c76919713a8a9cd54b2f', commit_message='Upload dataset', commit_description='', oid='0c2874fb4afc42bf8177c76919713a8a9cd54b2f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/DrSly/github-issues', endpoint='https://huggingface.co', repo_type='dataset', repo_id='DrSly/github-issues'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_with_comments_dataset.push_to_hub(\"github-issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "37fe034e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a746d1001fa54c0a900aeb8bc3a41ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3cba84a06a44bbda7d2d4aef66faf71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/9.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b2a8c9beab41f1b092e3ab399e9771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'type', 'active_lock_reason', 'sub_issues_summary', 'issue_dependencies_summary', 'body', 'closed_by', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'draft', 'pull_request', 'is_pull_request'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "remote_dataset = load_dataset(\"DrSly/github-issues\", split=\"train\")\n",
    "remote_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0000fe4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
