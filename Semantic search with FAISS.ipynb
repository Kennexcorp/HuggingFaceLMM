{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3386933",
   "metadata": {},
   "source": [
    "# Semantic search with FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b851f234",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Loading and preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d65ded2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'type', 'active_lock_reason', 'sub_issues_summary', 'issue_dependencies_summary', 'body', 'closed_by', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'draft', 'pull_request', 'is_pull_request'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "issues_dataset = load_dataset(\"DrSly/github-issues\", split=\"train\")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bbabbb",
   "metadata": {},
   "source": [
    "The first order of business is to filter out the pull requests, as these tend to be rarely used for answering user queries and will introduce noise in our search engine. let‚Äôs also filter out rows with no comments, since these provide no answers to user queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c38d36aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'type', 'active_lock_reason', 'sub_issues_summary', 'issue_dependencies_summary', 'body', 'closed_by', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'draft', 'pull_request', 'is_pull_request'],\n",
       "    num_rows: 1934\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset = issues_dataset.filter(\n",
    "    lambda x: (x['is_pull_request'] == False and len(x['comments']) > 0)\n",
    ")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2f7978",
   "metadata": {},
   "source": [
    "We can see that there are a lot of columns in our dataset, most of which we don‚Äôt need to build our search engine. From a search perspective, the most informative columns are title, body, and comments, while html_url provides us with a link back to the source issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c56ad2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 1934\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = issues_dataset.column_names\n",
    "columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
    "issues_dataset = issues_dataset.remove_columns(columns_to_remove)\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fc3bc1",
   "metadata": {},
   "source": [
    "To create our embeddings we‚Äôll augment each comment with the issue‚Äôs title and body, since these fields often include useful contextual information. Because our comments column is currently a list of comments for each issue, we need to ‚Äúexplode‚Äù the column so that each row consists of an (html_url, title, body, comment) tuple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed30fbf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I suggest metion this in docs specifically for attention with use, tell users explicitly to pass arguments with `fn_kwargs` param or using `functools.partial` to create a pure funcion.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset.set_format(\"pandas\")\n",
    "df = issues_dataset[:]\n",
    "df = df.fillna(\"\") \n",
    "df[\"comments\"][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa88e2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>`Dataset.map()` causes cache miss/fingerprint ...</td>\n",
       "      <td>I suggest metion this in docs specifically for...</td>\n",
       "      <td>### Describe the bug\\n\\nWhen using `.map()` wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>cast_column(..., Audio) fails with load_datase...</td>\n",
       "      <td>The following code *does* work:\\n```py\\nfrom d...</td>\n",
       "      <td>### Describe the bug\\n\\nAttempt to load a data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>cast_column(..., Audio) fails with load_datase...</td>\n",
       "      <td>Thanks for reporing ! Are you using pandas v3 ...</td>\n",
       "      <td>### Describe the bug\\n\\nAttempt to load a data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>cast_column(..., Audio) fails with load_datase...</td>\n",
       "      <td>pandas 3.0.0 was present but I've also reprodu...</td>\n",
       "      <td>### Describe the bug\\n\\nAttempt to load a data...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues...   \n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                               title  \\\n",
       "0  `Dataset.map()` causes cache miss/fingerprint ...   \n",
       "1  cast_column(..., Audio) fails with load_datase...   \n",
       "2  cast_column(..., Audio) fails with load_datase...   \n",
       "3  cast_column(..., Audio) fails with load_datase...   \n",
       "\n",
       "                                            comments  \\\n",
       "0  I suggest metion this in docs specifically for...   \n",
       "1  The following code *does* work:\\n```py\\nfrom d...   \n",
       "2  Thanks for reporing ! Are you using pandas v3 ...   \n",
       "3  pandas 3.0.0 was present but I've also reprodu...   \n",
       "\n",
       "                                                body  \n",
       "0  ### Describe the bug\\n\\nWhen using `.map()` wi...  \n",
       "1  ### Describe the bug\\n\\nAttempt to load a data...  \n",
       "2  ### Describe the bug\\n\\nAttempt to load a data...  \n",
       "3  ### Describe the bug\\n\\nAttempt to load a data...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df = df.explode(\"comments\", ignore_index=True)\n",
    "comments_df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117d702f",
   "metadata": {},
   "source": [
    "Great, we can see the rows have been replicated, with the comments column containing the individual comments! Now that we‚Äôre finished with Pandas, we can quickly switch back to a Dataset by loading the DataFrame in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cef6d04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 7255\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "comments_dataset = Dataset.from_pandas(comments_df)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b8ee88",
   "metadata": {},
   "source": [
    "Now that we have one comment per row, let‚Äôs create a new comments_length column that contains the number of words per comment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ef5acd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c7cfe515b34d3484ebd3b7f1b6f951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7255 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.map(lambda x: {\"comment_length\": len(x[\"comments\"].split())})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab33603c",
   "metadata": {},
   "source": [
    "We can use this new column to filter out short comments, which typically include things like ‚Äúcc @lewtun‚Äù or ‚ÄúThanks!‚Äù that are not relevant for our search engine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db0aa29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ee7f80112849f793871fb1298cdc6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7255 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
       "    num_rows: 5273\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.filter(lambda x: x['comment_length'] > 15)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1263aaca",
   "metadata": {},
   "source": [
    "Having cleaned up our dataset a bit, let‚Äôs concatenate the issue title, description, and comments together in a new text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd242bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d48510836e43bfa0c73e794793975d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5273 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def concatenate_text(examples):\n",
    "    return {\n",
    "        \"text\": examples[\"title\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"body\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"comments\"]\n",
    "    }\n",
    "\n",
    "\n",
    "comments_dataset = comments_dataset.map(concatenate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085520b7",
   "metadata": {},
   "source": [
    "## Creating text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f40ad18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc74ebc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d94e4a",
   "metadata": {},
   "source": [
    "As we mentioned earlier, we‚Äôd like to represent each entry in our GitHub issues corpus as a single vector, so we need to ‚Äúpool‚Äù or average our token embeddings in some way. One popular approach is to perform CLS pooling on our model‚Äôs outputs, where we simply collect the last hidden state for the special [CLS] token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2d33ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e50dea1",
   "metadata": {},
   "source": [
    "Next, we‚Äôll create a helper function that will tokenize a list of documents, place the tensors on the GPU, feed them to the model, and finally apply CLS pooling to the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b054cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6131a00c",
   "metadata": {},
   "source": [
    "We can test the function works by feeding it the first text entry in our corpus and inspecting the output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a96a406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = get_embeddings(comments_dataset[\"text\"][0])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e9e05b",
   "metadata": {},
   "source": [
    "Great, we‚Äôve converted the first entry in our corpus into a 768-dimensional vector! We can use Dataset.map() to apply our get_embeddings() function to each row in our corpus, so let‚Äôs create a new embeddings column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfed0694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b12c7ec7bf4be595f6dd867cd8aaa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5273 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_dataset = comments_dataset.map(\n",
    "    lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf0ed1d",
   "metadata": {},
   "source": [
    "we‚Äôve converted the embeddings to NumPy arrays ‚Äî that‚Äôs because ü§ó Datasets requires this format when we try to index them with FAISS\n",
    "\n",
    "### Using FAISS for efficient similarity search\n",
    "\n",
    "Now that we have a dataset of embeddings, we need some way to search over them. To do this, we‚Äôll use a special data structure in ü§ó Datasets called a FAISS index. FAISS (short for Facebook AI Similarity Search) is a library that provides efficient algorithms to quickly search and cluster embedding vectors.\n",
    "\n",
    "The basic idea behind FAISS is to create a special data structure called an index that allows one to find which embeddings are similar to an input embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99c41cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3c3fc39e164c00926211cf199f6fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text', 'embeddings'],\n",
       "    num_rows: 5273\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0712e1",
   "metadata": {},
   "source": [
    "We can now perform queries on this index by doing a nearest neighbor lookup with the Dataset.get_nearest_examples() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4417f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How can i load a dataset from the Hugging Face Hub offline?\"\n",
    "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
    "question_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacd6067",
   "metadata": {},
   "source": [
    "Just like with the documents, we now have a 768-dimensional vector representing the query, which we can compare against the whole corpus to find the most similar embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7eba62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, samples = embeddings_dataset.get_nearest_examples(\n",
    "    \"embeddings\", question_embedding, k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d34c9c0",
   "metadata": {},
   "source": [
    "The Dataset.get_nearest_examples() function returns a tuple of scores that rank the overlap between the query and the document, and a corresponding set of samples (here, the 5 best matches). Let‚Äôs collect these in a pandas.DataFrame so we can easily sort them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87982782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "samples_df[\"scores\"] = scores\n",
    "samples_df.sort_values(\"scores\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7043ed4a",
   "metadata": {},
   "source": [
    "Now we can iterate over the first few rows to see how well our query matched the available comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33929232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMENT: Hi ! The `gen_kwargs` dictionary is passed to `_generate_examples`, so in your case it must be defined this way:\n",
      "```python\n",
      "def _generate_examples(self, filepath):\n",
      "    ...\n",
      "```\n",
      "\n",
      "And here is an additional tip: you can use `os.path.join(downloaded_file, \"dataset/testing_data\")` instead of `f\"downloaded_file}/dataset/testing_data/\"` to get compatibility with Windows and streaming.\n",
      "\n",
      "Indeed Windows uses a backslash separator, not a slash, and streaming uses chained URLs (like `zip://dataset/testing_data::https://https://guillaumejaume.github.io/FUNSD/dataset.zip` for example)\n",
      "SCORE: 29.43366813659668\n",
      "TITLE: ‚ùì Dataset loading script from Hugging Face Hub\n",
      "URL: https://github.com/huggingface/datasets/issues/3300\n",
      "==================================================\n",
      "\n",
      "COMMENT: Also I think the viewer will be updated when you fix the dataset script, let me know if it doesn't\n",
      "SCORE: 29.401784896850586\n",
      "TITLE: ‚ùì Dataset loading script from Hugging Face Hub\n",
      "URL: https://github.com/huggingface/datasets/issues/3300\n",
      "==================================================\n",
      "\n",
      "COMMENT: Thanks for you quick reply @lhoestq and so sorry for my very delayed response.\n",
      "We have gotten around the error another way but I will try to duplicate this when I can.  We may have had \"filepaths\" instead of \"filepath\" in our def of _generate_examples() and not noticed the difference.  If I find a more useful answer for others I will add to this ticket so they know what the issue was.\n",
      "Note: we do have our own _generate_examples() defined with the same def as Quentin has.  (But one version does have \"filepaths\".)\n",
      "\n",
      "SCORE: 29.383466720581055\n",
      "TITLE: ‚ùì Dataset loading script from Hugging Face Hub\n",
      "URL: https://github.com/huggingface/datasets/issues/3300\n",
      "==================================================\n",
      "\n",
      "COMMENT: @lhoestq  I think I am having a related problem.\n",
      "My call to load_dataset() looks like this:\n",
      "\n",
      "```\n",
      "    datasets = load_dataset(\n",
      "        os.path.abspath(layoutlmft.data.datasets.xfun.__file__),\n",
      "        f\"xfun.{data_args.lang}\",\n",
      "        additional_langs=data_args.additional_langs,\n",
      "        keep_in_memory=True,\n",
      "    )\n",
      "\n",
      "```\n",
      "\n",
      "My _split_generation code is:\n",
      "\n",
      "```\n",
      "    def _split_generators(self, dl_manager):\n",
      "        \"\"\"Returns SplitGenerators.\"\"\"\n",
      "\n",
      "        downloaded_file = dl_manager.download_and_extract(\"https://guillaumejaume.github.io/FUNSD/dataset.zip\")\n",
      "        return [\n",
      "            datasets.SplitGenerator(\n",
      "                name=datasets.Split.TRAIN, gen_kwargs={\"filepath\": f\"{downloaded_file}/dataset/training_data/\"}\n",
      "            ),\n",
      "            datasets.SplitGenerator(\n",
      "                name=datasets.Split.TEST, gen_kwargs={\"filepath\": f\"{downloaded_file}/dataset/testing_data/\"}\n",
      "            ),\n",
      "        ]\n",
      "\n",
      "```\n",
      "However I get the error \"TypeError: _generate_examples() got an unexpected keyword argument 'filepath'\"\n",
      "The path looks right and I see the data in the path so I think the only problem I have is that it doesn't like the key \"filepath\".  However, the documentation (example [here](https://huggingface.co/datasets/lhoestq/custom_squad/blob/main/custom_squad.py#L101-L107)) seems to show that this is the correct parameter. \n",
      "\n",
      "Here is the full stack trace:\n",
      "\n",
      "```\n",
      "Downloading and preparing dataset xfun/xfun.en (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /Users/caseygre/.cache/huggingface/datasets/xfun/xfun.en/0.0.0/96b8cb7c57f6f822f0ab37ae3be7b82d84ac57062e774c9361ccf0a4b9ef61cc...\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/caseygre/PycharmProjects/aegis-ml-new/unilm/venv-LayoutLM/lib/python3.9/site-packages/datasets/builder.py\", line 574, in download_and_prepare\n",
      "    self._download_and_prepare(\n",
      "  File \"/Users/caseygre/PycharmProjects/aegis-ml-new/unilm/venv-LayoutLM/lib/python3.9/site-packages/datasets/builder.py\", line 652, in _download_and_prepare\n",
      "    self._prepare_split(split_generator, **prepare_split_kwargs)\n",
      "  File \"/Users/caseygre/PycharmProjects/aegis-ml-new/unilm/venv-LayoutLM/lib/python3.9/site-packages/datasets/builder.py\", line 975, in _prepare_split\n",
      "    generator = self._generate_examples(**split_generator.gen_kwargs)\n",
      "TypeError: _generate_examples() got an unexpected keyword argument 'filepath'\n",
      "python-BaseException\n",
      "```\n",
      "SCORE: 29.27052879333496\n",
      "TITLE: ‚ùì Dataset loading script from Hugging Face Hub\n",
      "URL: https://github.com/huggingface/datasets/issues/3300\n",
      "==================================================\n",
      "\n",
      "COMMENT: It should be easier to implement now that we improved the caching of datasets from `push_to_hub`: each dataset has its own directory in the cache.\n",
      "\n",
      "The cache structure has been improved in https://github.com/huggingface/datasets/pull/5331. Now the cache structure is `\"{namespace__}<dataset_name>/<config_name>/<version>/<hash>/\"` which contains the arrow files `\"<dataset_name>-<split>.arrow\"` and `\"dataset_info.json\"`. \n",
      "\n",
      "The idea is to extend `CachedDatasetModuleFactory` to also check if this directory exists in the cache (in addition to the already existing cache check) and return the requested dataset module. The module name can be found in the JSON file in the `builder_name` field.\n",
      "SCORE: 28.861364364624023\n",
      "TITLE: Datasets created with `push_to_hub` can't be accessed in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/3547\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _, row in samples_df.iterrows():\n",
    "    print(f\"COMMENT: {row.comments}\")\n",
    "    print(f\"SCORE: {row.scores}\")\n",
    "    print(f\"TITLE: {row.title}\")\n",
    "    print(f\"URL: {row.html_url}\")\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
