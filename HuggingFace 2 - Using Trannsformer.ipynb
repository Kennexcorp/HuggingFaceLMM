{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c78c3b5",
   "metadata": {},
   "source": [
    "## Behind the pipeline()\n",
    "Transformer models can't process raw texts directly, so the first step is to convert the text inputs into numbers which the model can understand through a process called tokenization using a tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f399cb8b",
   "metadata": {},
   "source": [
    "### Preprocessing - Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94fecc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf8a746",
   "metadata": {},
   "source": [
    "Next We pass our sentences to the tokenizer and feed the returned dictionary to model. \n",
    "Transformer models only accept tensors (plain numpy or PyTouch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd393975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  1037,  5470,  1997,  5087,  2142,\n",
       "          2005,  2086,   999,   102,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  1996,  2047,  3185,  2001,  1037, 10392,  6172,  2007, 14726,\n",
       "         26749,  1012,  1045,  3811, 16755,  2009,  2000,  3071,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_inputs: list[str] = [\n",
    "    \"I've been a fan of Manchester United for years!\",\n",
    "    \"The new movie was a fantastic adventure with stunning visuals. I highly recommend it to everyone.\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135b3de3",
   "metadata": {},
   "source": [
    "### Model\n",
    "This architecture contains only the base Transformer module: given some inputs, it outputs what we’ll call hidden states, also known as features. For each model input, we’ll retrieve a high-dimensional vector representing the contextual understanding of that input by the Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e4f8200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2ebb29",
   "metadata": {},
   "source": [
    "size = [batchsize, sequence_length, hidden_size]\n",
    "\n",
    "Batch size: The number of sequences processed at a time (2 in our example).\n",
    "Sequence length: The length of the numerical representation of the sequence (16 in our example).\n",
    "Hidden size: The vector dimension of each model input.\n",
    "\n",
    "It is said to be “high dimensional” because of the last value. The hidden size can be very large (768 is common for smaller models, and in larger models this can reach 3072 or more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3236effa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 20, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926771cf",
   "metadata": {},
   "source": [
    "Now if we look at the shape of our outputs, the dimensionality will be much lower: the model head takes as input the high-dimensional vectors we saw before, and outputs vectors containing two values (one per label):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4195bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b73510",
   "metadata": {},
   "source": [
    "### Postprocessing the output\n",
    "Our model predicted [-1.5607, 1.6123] for the first sentence and [ 4.1692, -3.3464] for the second one. Those are not probabilities but logits, the raw, unnormalized scores outputted by the last layer of the model. To be converted to probabilities, they need to go through a SoftMax layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a16fdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.5853,  3.8333],\n",
      "        [-4.3617,  4.7374]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6226ff0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.9962e-04, 9.9940e-01],\n",
       "        [1.1176e-04, 9.9989e-01]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a94bbd0",
   "metadata": {},
   "source": [
    "Now we can see that the model predicted [0.000599, 0.994] for the first sentence and [0.000111, 0.9999] for the second one. These are recognizable probability scores. To get the labels corresponding to each position, we can inspect the id2label attribute of the model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55fc5020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30836cc",
   "metadata": {},
   "source": [
    "We have successfully reproduced the three steps of the pipeline: preprocessing with tokenizers, passing the inputs through the model, and postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd44b59",
   "metadata": {},
   "source": [
    "# Models\n",
    "Explore creating models using AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac0a458",
   "metadata": {},
   "source": [
    "### Creating a transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4b07d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "# download model\n",
    "model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# save model\n",
    "model.save_pretrained('Models/bert-base-cased')\n",
    "\n",
    "# load model\n",
    "model = AutoModel.from_pretrained('Models/bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9de1a2",
   "metadata": {},
   "source": [
    "### Encoding text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9d8524b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 8667, 117, 4373, 1128, 6407, 2052, 136, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "encoded_input = tokenizer(\"Hello, Have you sang today?\")\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce564d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Hello, Have you sang today? [SEP]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can decode the input IDs to get back the original text\n",
    "tokenizer.decode(encoded_input['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cd2bc86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   146,   112,  1396,  1151,   170,  5442,  1104,  4280,  1244,\n",
       "          1111,  1201,   106,   102,     0,     0],\n",
       "        [  101,  1109,  1207,  2523,  1108,   170, 14820,  7644,  1114, 15660,\n",
       "          5173,  1116,   119,   146,  3023,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode multiple sentences at once\n",
    "sequences: list[str] = [\n",
    "    \"I've been a fan of Manchester United for years!\",\n",
    "    \"The new movie was a fantastic adventure with stunning visuals. I highly recommend it to everyone.\",\n",
    "]\n",
    "inputs = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\", max_length=16)\n",
    "inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38a8f766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] I ' ve been a fan of Manchester United for years! [SEP] [PAD] [PAD]\n",
      "[CLS] The new movie was a fantastic adventure with stunning visuals. I highly [SEP]\n"
     ]
    }
   ],
   "source": [
    "for ids in inputs['input_ids']:\n",
    "    print(tokenizer.decode(ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b69d6e",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73914181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 1005, 2310, 2042, 1037, 5470, 1997, 5087, 2142, 2005, 2086, 999, 102], [101, 1999, 2035, 2026, 2086, 1999, 2326, 1045, 2031, 2196, 8828, 9781, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequences = [\"I've been a fan of Manchester United for years!\", \"In all my years in service i have never eaten corn\"]\n",
    "\n",
    "tokens = tokenizer(sequences)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6cb23c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2819, -0.0083,  0.0651,  ..., -0.2407,  0.4291,  0.1038],\n",
       "         [-0.1350, -0.6947,  0.2639,  ..., -0.1382,  0.6238,  0.6530],\n",
       "         [-0.3829, -0.9847,  0.5163,  ...,  0.0986,  0.4749,  0.7193],\n",
       "         ...,\n",
       "         [-0.2084, -0.4938,  0.2015,  ...,  0.1113,  0.4608,  0.2006],\n",
       "         [ 0.1782, -0.2340,  0.2628,  ..., -0.4469,  0.1198,  0.2138],\n",
       "         [ 0.6864, -0.7907,  0.0539,  ..., -0.9187,  0.7601, -0.0813]],\n",
       "\n",
       "        [[ 0.4178,  0.0213,  0.0459,  ..., -0.2817,  0.1344, -0.0787],\n",
       "         [ 0.0534, -0.2096,  0.1823,  ...,  0.0932, -0.0687, -0.3376],\n",
       "         [-0.1404, -0.1754,  0.1438,  ...,  0.3340, -0.0152,  0.1059],\n",
       "         ...,\n",
       "         [ 0.2635, -0.3036, -0.0576,  ..., -0.2675,  0.1084, -0.0407],\n",
       "         [ 0.6577,  0.4530,  0.2398,  ..., -0.7735,  0.3696, -1.0877],\n",
       "         [-0.0570, -0.2286,  0.0018,  ...,  0.3271, -0.0414,  0.0727]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.6826,  0.5216,  0.9999,  ...,  1.0000, -0.4293,  0.9867],\n",
       "        [-0.5904,  0.4030,  0.9997,  ...,  0.9999, -0.7999,  0.9763]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can pad, truncate, and set a max length\n",
    "tokens = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ec7c038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " \"'\",\n",
       " 've',\n",
       " 'been',\n",
       " 'a',\n",
       " 'fan',\n",
       " 'of',\n",
       " 'manchester',\n",
       " 'united',\n",
       " 'for',\n",
       " 'years',\n",
       " '!']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"I've been a fan of Manchester United for years!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d383ca17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
